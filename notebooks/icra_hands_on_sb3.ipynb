{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnjnsng/ICRA2022-Tutorial/blob/main/notebooks/icra_hands_on_sb3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Gym/Stable Baselines3 Getting Started - ICRA 2022\n",
        "\n",
        "Github repo: https://github.com/araffin/tools-for-robotic-rl-icra2022\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "You will also learn how to define a gym wrapper to customise the training.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gWskDE2c9WoN",
        "vscode": {
          "languageId": "python"
        },
        "outputId": "98b1f62f-7472-485a-9494-3cb9aa626da1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 0s (2,124 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hYdv2ygjLaFL",
        "vscode": {
          "languageId": "python"
        },
        "outputId": "b1ffb236-44d4-4408-e2dd-428e201bd4ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 177 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3[extra]) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.46.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616828 sha256=be169d7afa418d9a0e4073d6fe99ccb52d8b84021da039e4c0a60ec088b88a8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=0887b949d0aa9bb850794ff9f639c84f6a5395255afd809c9a8e2eb7ac9a5a67\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oexj67yWN5_k",
        "vscode": {
          "languageId": "python"
        },
        "outputId": "7d0bba4a-0fd6-4652-d53a-4ce66cde53b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sb3-contrib\n",
            "  Downloading sb3_contrib-1.5.0-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: stable-baselines3>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from sb3-contrib) (1.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (1.11.0+cu113)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (1.21.6)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.5.0->sb3-contrib) (0.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3>=1.5.0->sb3-contrib) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3>=1.5.0->sb3-contrib) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3>=1.5.0->sb3-contrib) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.5.0->sb3-contrib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.5.0->sb3-contrib) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.5.0->sb3-contrib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.5.0->sb3-contrib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3>=1.5.0->sb3-contrib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3>=1.5.0->sb3-contrib) (2022.1)\n",
            "Installing collected packages: sb3-contrib\n",
            "Successfully installed sb3-contrib-1.5.0\n"
          ]
        }
      ],
      "source": [
        "# Optional: install SB3 contrib to have access to additional algorithms\n",
        "!pip install sb3-contrib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNkrgcI6Z1"
      },
      "source": [
        "# Part I: Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kh7qCYtAES4v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the Gym interface\n",
        "\n",
        "An environment that follows the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) is quite simple to use.\n",
        "It provides to this user mainly three methods:\n",
        "- `reset()` called at the beginning of an episode, it returns an observation\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
        "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
        "\n",
        "Below you can find an example of a custom environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owrHOGhlMGLE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, NamedTuple, Tuple, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "GymObs = Union[Tuple, Dict, np.ndarray, int]\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Minimal custom environment to demonstrate the Gym interface.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(14,))\n",
        "    self.action_space = gym.spaces.Box(low=-1, high=1, shape=(6,))\n",
        "\n",
        "  def reset(self) -> GymObs:\n",
        "    \"\"\"\n",
        "    Called at the beginning of an episode.\n",
        "    :return: the first observation of the episode\n",
        "    \"\"\"\n",
        "    return self.observation_space.sample()\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    \"\"\"\n",
        "    Step into the environment.\n",
        "    :return: A tuple containing the new observation, the reward signal, \n",
        "      whether the episode is over and additional informations.\n",
        "    \"\"\"\n",
        "    obs = self.observation_space.sample()\n",
        "    reward = 1.0\n",
        "    done = False\n",
        "    # Whether the termination was due to timeout or not\n",
        "    info = {\"TimeLimit.truncated\": False}\n",
        "    return obs, reward, done, info\n",
        "\n",
        "env = CustomEnv()\n",
        "# Check your custom environment\n",
        "# this will print warnings and throw errors if needed\n",
        "check_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://www.gymlibrary.ml/).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym document is still a work in progress.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7tKaBFrTR0a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcsXmYRMON9W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Algorithms from the contrib repo\n",
        "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "from sb3_contrib import QRDQN, TQC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO(\"MlpPolicy\", env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUJr675TT01",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use Pendulum environment, a classic control problem.\n",
        "\n",
        "\"The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\"\n",
        "\n",
        "Pendulum environment: [https://www.gymlibrary.ml/environments/classic_control/pendulum/](https://www.gymlibrary.ml/environments/classic_control/pendulum/)\n",
        "\n",
        "\n",
        "![Pendulum-v1](https://huggingface.co/sb3/ppo-Pendulum-v1/resolve/main/pendulum.gif)\n",
        "\n",
        "We chose the MlpPolicy because the observation of the Pendulum task is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://sb3-contrib.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create the gym Env\n",
        "env_id = \"Pendulum-v1\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the RL agent\n",
        "# Here we are using tuned hyperparameters\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    gamma=0.98,\n",
        "    use_sde=True,\n",
        "    sde_sample_freq=4,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K2YzpGDwQ4"
      },
      "source": [
        "### Using the model to predict actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfb_f7uD-H9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF_xsDEADvv_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve first observation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgrRoM23EU9Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict the action to take given the observation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1wRSvPrEjUa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# We are using continuous actions, therefore `action` is a numpy array\n",
        "assert env.action_space.contains(action)\n",
        "\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtEtKqKEvFC"
      },
      "source": [
        "Step in the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJ-yv5MEllY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTNOOlOdE1lH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"obs_shape={obs.shape}, reward={reward}, done? {done}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dd6hT873ImE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Reset the env at the end of an episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSeXppUFNLg"
      },
      "source": [
        "### Exercise (10 minutes): write the function to evaluate the agent\n",
        "\n",
        "This function will be used to evaluate the performance of an RL agent.\n",
        "Thanks to Stable Baselines3 interface, it will work with any SB3 algorithms and any Gym environment.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf7Ktha2FE3B",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    env: gym.Env,\n",
        "    n_eval_episodes: int = 100,\n",
        "    deterministic: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate an RL agent for `n_eval_episodes`.\n",
        "\n",
        "    :param model: the RL Agent\n",
        "    :param env: the gym Environment\n",
        "    :param n_eval_episodes: number of episodes to evaluate it\n",
        "    :param deterministic: Whether to use deterministic or stochastic actions\n",
        "    :return: Mean episodic reward for the last `n_eval_episodes`\n",
        "     (Mean over episodes of the cumulative episodic reward)\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO: run `n_eval_episodes` episodes in the Gym env\n",
        "    # using the RL agent and keep track of the total reward\n",
        "    # collected for each episode (aka cumulative reward or episode return).\n",
        "    # Finally, compute the mean and print it\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3SahgtZH2TX"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGmJK4oHUeT",
        "outputId": "f3b1fbf7-d6c3-470c-e6ae-8e61a3fe4be6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 21.45 Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, env, n_eval_episodes=20, deterministic=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines already provides you with that helper (the actual implementation is a little more advanced):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z6K9YImYJEx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lHSorUyH8I8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# The Monitor wrapper allows to keep track of the training reward and other infos (useful for plotting)\n",
        "env = Monitor(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oPTHjxyZSOL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Seed to compare to previous implementation\n",
        "# env.reset(seed=42) with gym 0.23\n",
        "env.seed(42)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Train the agent for 50 000 steps\n",
        "model.learn(total_timesteps=50_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyfQxD5z26J",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you can learn more about those wrappers in our Documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "record_video('Pendulum-v1', model, video_length=500, prefix='ppo-pendulum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "show_videos('videos', prefix='ppo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5WHto5hNaZN"
      },
      "source": [
        "### [Optional] Exercise (5 minutes): Save, Load The Model and that the loading was correct\n",
        "\n",
        "Save the model and then load it.\n",
        "\n",
        "Don't forget to check that loading went well: the model must predict the same actions given the same  observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCcS6wJ6Nurd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sample observations using the environment observation space\n",
        "\n",
        "# Predict actions on those observations using trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWCUZSD7NZ_Y",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2M1S_-V2gMi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Delete the model (to demonstrate loading)\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ne1MlIiNpee",
        "outputId": "0350fd76-db7b-4943-c7e1-9bfa9a4c7ddd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ppo_cartpole.zip\n"
          ]
        }
      ],
      "source": [
        "!ls *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55G-6nJLNrPk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIWII5YxNtrI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict actions on the observations with the loaded model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m06ipphZN_0E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Check that the predictions are the same\n",
        "assert np.allclose(action_before_saving, action_after_loading), \"Somethng went wrong in the loading\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm4RncfCJFyP"
      },
      "source": [
        "# Part II: Gym Wrappers\n",
        "\n",
        "\n",
        "In this part, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYo0C0TQSL3c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env:  Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, infos = self.env.step(action)\n",
        "    return obs, reward, done, infos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RkTLshPMGq"
      },
      "source": [
        "### [Optional] Exercise (7 minutes): limit the episode length\n",
        "\n",
        "In this exercise, the goal is to create a Gym wrapper that will limit the maximum number of steps per episode (timeout).\n",
        "\n",
        "\n",
        "It will also pass a `timeout` signal in the info dict to tell the agent that the termination was due to reaching the limits.\n",
        "\n",
        "Termination due to timeout must be handled separately (see [Time Limit in RL paper](https://arxiv.org/abs/1712.00378)), you can also take a look at [Issue #284](https://github.com/DLR-RM/stable-baselines3/issues/284) and [Issue #633](https://github.com/DLR-RM/stable-baselines3/issues/633)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_ypIxgPLGL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Limit the maximum number of steps per episode.\n",
        "\n",
        "  :param env: Gym environment that will be wrapped\n",
        "  :param max_steps: Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env, max_steps: int = 100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # YOUR CODE HERE\n",
        "    # Counter of steps per episode\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "  \n",
        "  def reset(self) -> GymObs:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: reset the counter and reset the env\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: \n",
        "    # 1. Step into the env\n",
        "    # 2. Increment the episode counter\n",
        "    # 3. Overwrite the done signal when time limit is reached \n",
        "    # (optional) 4. update the info dict (add a \"episode_timeout\" key)\n",
        "    # when the episode was stopped due to timelimit\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs, reward, done, infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQptLSuPB3A"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szZ43D5PVB07",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cencka9iVg9V",
        "outputId": "01ddfdb8-be4c-4df9-e052-d4a54666419e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode length: 100 steps, info dict: {'episode_timeout': True}\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, infos = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(f\"Episode length: {n_steps} steps, info dict: {infos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "What we have seen in this notebook:\n",
        "- SB3 101\n",
        "- Gym wrappers to modify the env\n",
        "- more complete tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "- longer hands-on session: https://www.youtube.com/watch?v=Ikngt0_DXJg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-gqIPXqV7zZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "icra22_hands_on_sb3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}